"use strict";(self.webpackChunkdocassemble_assembly_line_documentation=self.webpackChunkdocassemble_assembly_line_documentation||[]).push([[291],{4137:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>f});var a=n(7294);function l(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){l(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function o(e,t){if(null==e)return{};var n,a,l=function(e,t){if(null==e)return{};var n,a,l={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(l[n]=e[n]);return l}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(l[n]=e[n])}return l}var r=a.createContext({}),p=function(e){var t=a.useContext(r),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},u=function(e){var t=p(e.components);return a.createElement(r.Provider,{value:t},e.children)},m="mdxType",d={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,l=e.mdxType,i=e.originalType,r=e.parentName,u=o(e,["components","mdxType","originalType","parentName"]),m=p(n),c=l,f=m["".concat(r,".").concat(c)]||m[c]||d[c]||i;return n?a.createElement(f,s(s({ref:t},u),{},{components:n})):a.createElement(f,s({ref:t},u))}));function f(e,t){var n=arguments,l=t&&t.mdxType;if("string"==typeof e||l){var i=n.length,s=new Array(i);s[0]=c;var o={};for(var r in t)hasOwnProperty.call(t,r)&&(o[r]=t[r]);o.originalType=e,o[m]="string"==typeof e?e:l,s[1]=o;for(var p=2;p<i;p++)s[p]=n[p];return a.createElement.apply(null,s)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},5811:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>r,contentTitle:()=>s,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>p});var a=n(7462),l=(n(7294),n(4137));const i={sidebar_label:"llms",title:"ALToolbox.llms"},s=void 0,o={unversionedId:"reference/ALToolbox/llms",id:"reference/ALToolbox/llms",title:"ALToolbox.llms",description:"chat\\_completion",source:"@site/docs/reference/ALToolbox/llms.md",sourceDirName:"reference/ALToolbox",slug:"/reference/ALToolbox/llms",permalink:"/docassemble-AssemblyLine-documentation/docs/reference/ALToolbox/llms",draft:!1,editUrl:"https://github.com/SuffolkLITLab/docassemble-AssemblyLine-documentation/edit/main/docs/reference/ALToolbox/llms.md",tags:[],version:"current",frontMatter:{sidebar_label:"llms",title:"ALToolbox.llms"}},r={},p=[{value:"chat_completion",id:"chat_completion",level:4},{value:"extract_fields_from_text",id:"extract_fields_from_text",level:4},{value:"match_goals_from_text",id:"match_goals_from_text",level:4},{value:"classify_text",id:"classify_text",level:4},{value:"synthesize_user_responses",id:"synthesize_user_responses",level:4},{value:"define_fields_from_dict",id:"define_fields_from_dict",level:4},{value:"Goal Objects",id:"goal-objects",level:2},{value:"response_satisfies_me_or_follow_up",id:"response_satisfies_me_or_follow_up",level:4},{value:"get_next_question",id:"get_next_question",level:4},{value:"GoalDict Objects",id:"goaldict-objects",level:2},{value:"satisfied",id:"satisfied",level:4},{value:"GoalQuestion Objects",id:"goalquestion-objects",level:2},{value:"GoalSatisfactionList Objects",id:"goalsatisfactionlist-objects",level:2},{value:"mark_satisfied_goals",id:"mark_satisfied_goals",level:4},{value:"keep_going",id:"keep_going",level:4},{value:"need_more_questions",id:"need_more_questions",level:4},{value:"satisfied",id:"satisfied-1",level:4},{value:"get_next_goal_and_question",id:"get_next_goal_and_question",level:4},{value:"synthesize_draft_response",id:"synthesize_draft_response",level:4}],u={toc:p};function m(e){let{components:t,...n}=e;return(0,l.kt)("wrapper",(0,a.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,l.kt)("h4",{id:"chat_completion"},"chat","_","completion"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def chat_completion(\n    system_message: Optional[str] = None,\n    user_message: Optional[str] = None,\n    openai_client: Optional[OpenAI] = None,\n    openai_api: Optional[str] = None,\n    temperature: float = 0.5,\n    json_mode=False,\n    model: str = "gpt-3.5-turbo",\n    messages: Optional[List[Dict[str, str]]] = None\n) -> Union[List[Any], Dict[str, Any], str]\n')),(0,l.kt)("p",null,"A light wrapper on the OpenAI chat endpoint."),(0,l.kt)("p",null,"Includes support for token limits, error handling, and moderation queue."),(0,l.kt)("p",null,"It is also possible to specify an alternative model, and we support GPT-4-turbo","'","s JSON\nmode."),(0,l.kt)("p",null,"As of today (1/2/2024) JSON mode requires the model to be set to ",'"',"gpt-4-1106-preview",'"'," or ",'"',"gpt-3.5-turbo-1106",'"'),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"system_message")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The role the chat engine should play"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"user_message")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The message (data) from the user"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"openai_client")," ",(0,l.kt)("em",{parentName:"li"},"Optional","[OpenAI]")," - An OpenAI client object, optional. If omitted, will fall back to creating a new OpenAI client with the API key provided as an environment variable"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"openai_api")," ",(0,l.kt)("em",{parentName:"li"},"Optional","[str]")," - the API key for an OpenAI client, optional. If provided, a new OpenAI client will be created."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"temperature")," ",(0,l.kt)("em",{parentName:"li"},"float")," - The temperature to use for the GPT-4-turbo API"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"json_mode")," ",(0,l.kt)("em",{parentName:"li"},"bool")," - Whether to use JSON mode for the GPT-4-turbo API")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("p",null,"  A string with the response from the API endpoint or JSON data if json_mode is True"),(0,l.kt)("h4",{id:"extract_fields_from_text"},"extract","_","fields","_","from","_","text"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def extract_fields_from_text(text: str,\n                             field_list: Dict[str, str],\n                             openai_client: Optional[OpenAI] = None,\n                             openai_api: Optional[str] = None,\n                             temperature: float = 0,\n                             model="gpt-3.5-turbo-1106") -> Dict[str, Any]\n')),(0,l.kt)("p",null,"Extracts fields from text."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"text")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The text to extract fields from"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"field_list")," ",(0,l.kt)("em",{parentName:"li"},"Dict","[str,str]")," - A list of fields to extract, with the key being the field name and the value being a description of the field")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("p",null,"  A dictionary of fields extracted from the text"),(0,l.kt)("h4",{id:"match_goals_from_text"},"match","_","goals","_","from","_","text"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def match_goals_from_text(question: str,\n                          user_response: str,\n                          goals: Dict[str, str],\n                          openai_client: Optional[OpenAI] = None,\n                          openai_api: Optional[str] = None,\n                          temperature: float = 0,\n                          model="gpt-3.5-turbo-1106") -> Dict[str, Any]\n')),(0,l.kt)("p",null,"Read","'","s a user","'","s message and determines whether it meets a set of goals, with the help of an LLM."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"text")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The text to extract goals from"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"field_list")," ",(0,l.kt)("em",{parentName:"li"},"Dict","[str,str]")," - A list of goals to extract, with the key being the goal name and the value being a description of the goal")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("p",null,"  A dictionary of fields extracted from the text"),(0,l.kt)("h4",{id:"classify_text"},"classify","_","text"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def classify_text(text: str,\n                  choices: Dict[str, str],\n                  default_response: str = "null",\n                  openai_client: Optional[OpenAI] = None,\n                  openai_api: Optional[str] = None,\n                  temperature: float = 0,\n                  model="gpt-3.5-turbo-1106") -> str\n')),(0,l.kt)("p",null,"Given a text, classify it into one of the provided choices with the assistance of a large language model."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"text")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The text to classify"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"choices")," ",(0,l.kt)("em",{parentName:"li"},"Dict","[str,str]")," - A list of choices to classify the text into, with the key being the choice name and the value being a description of the choice"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"openai_client")," ",(0,l.kt)("em",{parentName:"li"},"Optional","[OpenAI]")," - An OpenAI client object, optional. If omitted, will fall back to creating a new OpenAI client with the API key provided as an environment variable"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"openai_api")," ",(0,l.kt)("em",{parentName:"li"},"Optional","[str]")," - the API key for an OpenAI client, optional. If provided, a new OpenAI client will be created."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"temperature")," ",(0,l.kt)("em",{parentName:"li"},"float")," - The temperature to use for GPT. Defaults to 0."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"model")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The model to use for the GPT API")),(0,l.kt)("h4",{id:"synthesize_user_responses"},"synthesize","_","user","_","responses"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def synthesize_user_responses(messages: List[Dict[str, str]],\n                              custom_instructions: Optional[str] = "",\n                              openai_client: Optional[OpenAI] = None,\n                              openai_api: Optional[str] = None,\n                              temperature: float = 0,\n                              model: str = "gpt-3.5-turbo-1106") -> str\n')),(0,l.kt)("p",null,"Given a first draft and a series of follow-up questions and answers, use an LLM to synthesize the user","'","s responses\ninto a single, coherent reply."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"custom_instructions")," ",(0,l.kt)("em",{parentName:"li"},"str")," - Custom instructions for the LLM to follow in constructing the synthesized response"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"initial_draft")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The initial draft of the response from the user"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"messages")," ",(0,l.kt)("em",{parentName:"li"},"List[Dict","[str, str]","]")," - A list of questions from the LLM and responses from the user"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"openai_client")," ",(0,l.kt)("em",{parentName:"li"},"Optional","[OpenAI]")," - An OpenAI client object, optional. If omitted, will fall back to creating a new OpenAI client with the API key provided as an environment variable"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"openai_api")," ",(0,l.kt)("em",{parentName:"li"},"Optional","[str]")," - the API key for an OpenAI client, optional. If provided, a new OpenAI client will be created."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"temperature")," ",(0,l.kt)("em",{parentName:"li"},"float")," - The temperature to use for GPT. Defaults to 0."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"model")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The model to use for the GPT API")),(0,l.kt)("h4",{id:"define_fields_from_dict"},"define","_","fields","_","from","_","dict"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def define_fields_from_dict(field_dict: Dict[str, Any],\n                            fields_to_ignore: Optional[List] = None) -> None\n")),(0,l.kt)("p",null,"Assigns the values in a dictionary of fields to the corresponding fields in a Docassemble interview."),(0,l.kt)("p",null,"Docassemble and built-in keywords are never defined by this function. If fields_to_ignore is provided, those fields will also be ignored."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"field_dict")," ",(0,l.kt)("em",{parentName:"li"},"Dict","[str, Any]")," - A dictionary of fields to define, with the key being the field name and the value\npresumably taken from the output of extract_fields_from_text."),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"fields_to_ignore")," ",(0,l.kt)("em",{parentName:"li"},"Optional","[List]")," - A list of fields to ignore. Defaults to None. Should be used to ensure\nsafety when defining fields from untrusted sources. E.g., ",'["',"user_is_logged_in",'"]')),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("p",null,"  None"),(0,l.kt)("h2",{id:"goal-objects"},"Goal Objects"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"class Goal(DAObject)\n")),(0,l.kt)("p",null,"A class to represent a goal."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Attributes"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"name")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The name of the goal"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"description")," ",(0,l.kt)("em",{parentName:"li"},"str")," - A description of the goal"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"satisfied")," ",(0,l.kt)("em",{parentName:"li"},"bool")," - Whether the goal is satisfied")),(0,l.kt)("h4",{id:"response_satisfies_me_or_follow_up"},"response","_","satisfies","_","me","_","or","_","follow","_","up"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def response_satisfies_me_or_follow_up(messages: List[Dict[str, str]],\n                                       openai_client: Optional[OpenAI] = None,\n                                       model="gpt-3.5-turbo") -> str\n')),(0,l.kt)("p",null,"Returns the text of the next question to ask the user or the string ",'"',"satisfied",'"',"\nif the user","'","s response satisfies the goal."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Arguments"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"response")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The response to check")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("p",null,"  True if the response satisfies the goal, False otherwise"),(0,l.kt)("h4",{id:"get_next_question"},"get","_","next","_","question"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},'def get_next_question(thread_so_far: List[Dict[str, str]],\n                      openai_client: Optional[OpenAI] = None,\n                      model="gpt-3.5") -> str\n')),(0,l.kt)("p",null,"Returns the text of the next question to ask the user."),(0,l.kt)("h2",{id:"goaldict-objects"},"GoalDict Objects"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"class GoalDict(DADict)\n")),(0,l.kt)("p",null,"A class to represent a DADict of Goals."),(0,l.kt)("h4",{id:"satisfied"},"satisfied"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def satisfied()\n")),(0,l.kt)("p",null,"Returns True if all goals are satisfied, False otherwise."),(0,l.kt)("h2",{id:"goalquestion-objects"},"GoalQuestion Objects"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"class GoalQuestion(DAObject)\n")),(0,l.kt)("p",null,"A class to represent a question about a goal."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Attributes"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"goal")," ",(0,l.kt)("em",{parentName:"li"},"Goal")," - The goal the question is about"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"question")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The question to ask the user"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"response")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The user","'","s response to the question")),(0,l.kt)("h2",{id:"goalsatisfactionlist-objects"},"GoalSatisfactionList Objects"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"class GoalSatisfactionList(DAList)\n")),(0,l.kt)("p",null,"A class to help ask the user questions until all goals are satisfied."),(0,l.kt)("p",null,"Uses an LLM to prompt the user with follow-up questions if the initial response isn","'","t complete.\nBy default, the number of follow-up questions is limited to 10."),(0,l.kt)("p",null,"This can consume a lot of tokens, as each follow-up has a chance to send the whole conversation\nthread to the LLM."),(0,l.kt)("p",null,"By default, this will use the OpenAI API key defined in the global configuration under this path:"),(0,l.kt)("p",null,"You can specify the path to an alternative configuration by setting the ",(0,l.kt)("inlineCode",{parentName:"p"},"openai_configuration_path")," attribute."),(0,l.kt)("p",null,"This object does NOT accept the key as a direct parameter, as that will be leaked in the user","'","s answers."),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre"},"open ai:\n    key: sk-...\n")),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Attributes"),":"),(0,l.kt)("ul",null,(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"goals")," ",(0,l.kt)("em",{parentName:"li"},"List","[Goal]")," - The goals in the list, provided as a dictionary"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"goal_list")," ",(0,l.kt)("em",{parentName:"li"},"GoalList")," - The list of Goals"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"question_limit")," ",(0,l.kt)("em",{parentName:"li"},"int")," - The maximum number of follow-up questions to ask the user"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"question_per_goal_limit")," ",(0,l.kt)("em",{parentName:"li"},"int")," - The maximum number of follow-up questions to ask the user per goal"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"initial_draft")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The initial draft of the user","'","s response"),(0,l.kt)("li",{parentName:"ul"},(0,l.kt)("inlineCode",{parentName:"li"},"initial_question")," ",(0,l.kt)("em",{parentName:"li"},"str")," - The original question posed in the interview")),(0,l.kt)("h4",{id:"mark_satisfied_goals"},"mark","_","satisfied","_","goals"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def mark_satisfied_goals() -> None\n")),(0,l.kt)("p",null,"Marks goals as satisfied if the user","'","s response satisfies the goal.\nThis should be used as soon as the user gives their initial reply."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("p",null,"  None"),(0,l.kt)("h4",{id:"keep_going"},"keep","_","going"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def keep_going()\n")),(0,l.kt)("p",null,"Returns True if there is at least one unsatisfied goal and if the number of follow-up questions asked is less than the question limit, False otherwise."),(0,l.kt)("h4",{id:"need_more_questions"},"need","_","more","_","questions"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def need_more_questions()\n")),(0,l.kt)("p",null,"Returns True if there is at least one unsatisfied goal, False otherwise."),(0,l.kt)("p",null,"Also has the side effect of checking the user","'","s most recent response to see if it satisfies the goal\nand updating the next question to be asked."),(0,l.kt)("h4",{id:"satisfied-1"},"satisfied"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def satisfied()\n")),(0,l.kt)("p",null,"Returns True if all goals are satisfied, False otherwise."),(0,l.kt)("h4",{id:"get_next_goal_and_question"},"get","_","next","_","goal","_","and","_","question"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def get_next_goal_and_question()\n")),(0,l.kt)("p",null,"Returns the next unsatisfied goal, along with a follow-up question to ask the user, if relevant."),(0,l.kt)("p",null,(0,l.kt)("strong",{parentName:"p"},"Returns"),":"),(0,l.kt)("p",null,"  A tuple of (Goal, str) where the first item is the next unsatisfied goal and the second item is the next question to ask the user, if relevant.\nIf the user","'","s response to the last question satisfied the goal, returns (None, None)."),(0,l.kt)("h4",{id:"synthesize_draft_response"},"synthesize","_","draft","_","response"),(0,l.kt)("pre",null,(0,l.kt)("code",{parentName:"pre",className:"language-python"},"def synthesize_draft_response()\n")),(0,l.kt)("p",null,"Returns a draft response that synthesizes the user","'","s responses to the questions."))}m.isMDXComponent=!0}}]);